# K均值聚类——K值的选取

## K均值聚类算法简要描述

K均值聚类算法是最简单的一种聚类算法。算法的目的是使**各个样本与所在类均值的误差平方和达到最小（这也是评价K-means算法最后聚类效果的评价标准）**

K均值聚类算法的一般步骤：

1. 初始化。输入基因表达矩阵作为对象集X，输入指定聚类类数*N*，并在X中随机选取*N*个对象作为初始聚类中心。设定迭代中止条件，比如最大循环次数或者聚类中心收敛误差容限。
2. 进行迭代。根据相似度准则将数据对象分配到最接近的聚类中心，从而形成一类。初始化隶属度矩阵。
3. 更新聚类中心。然后以每一类的平均向量作为新的聚类中心，重新分配数据对象。
4. 反复执行第二步和第三步直至满足中止条件。

伪代码如下：
```
创建k个作为起始质心（通常是随机选择）
当任意一个点的簇分配结果发生改变时
    对数据集中的每个数据点
        对每个质心
            计算质心与数据点之间的距离
        将数据点分配到距离其最近的簇
	对每一个簇，计算簇中所有点的均值并将均值作为质心
```
K均值聚类的一般流程：

1. 收集数据：使用任意方法

2. 准备数据：需要数据型数据来计算距离，也可以将标称型数据映射为二值型数据再用于距离计算。

3. 分析数据：使用任意方法。

4. 训练算法：不适用于无监督学习，即无监督学习没有训练过程。

5. 测试算法：应用聚类算法，观察结果。可以使用量化的误差指标如误差平方和来评价算法的结果。

6. 使用算法：可以用于所希望的任何应用，通常情况下，簇质心可以代表整个簇的数据来做出决策

## 最佳聚类数的确定方法

### 传统方法

传统的聚类数确定，是通过聚类有效性指标来评价不同k值下聚类结果的优劣，从而选出最优的聚类数。常用的聚类有效性指标有Calinski-Harabasz（CH）指标、Davies-Bouldin（DB）指标、Weighted inter-intra（Wint）指标、Krzanowski-Lai（KL）指标、Hartigan（Hart）指标、In- Group roportion（IGP）指标等。其中，IGP指标是基于数据集统计信息的指标，而其他指标全都是局域数据集样本集合机构的指标，他们不依赖外部的参考标准，只依据数据集本身的统计特征对聚类结果进行评估，并根据结果的优劣选取最佳聚类数。

以下介绍最常用的几个指标：

1. $CH$指标

   $CH$指标通过类内离差矩阵描述紧密度，类间离差矩阵描述分离度，指标定义为：
   $$
   CH(k)=\frac{trB(k)/(k-1)}{trW(k)/(n-k)}
   $$
   其中，$n$为聚类数目，$k$表示当前类，$trB(k)$表示类间离差矩阵的迹，$trW(k)$表示类内离差矩阵的迹。可以得出$CH$越大代表着类自身越紧密，类与类之间越分散，即更优的聚类结果。

2. DB指标

   DB指标是基于样本的类内散度与各聚类中心的间距的方法，其定义为：
   $$
   D B ( k ) = \frac { 1 } { k } \sum _ { l } ^ { k } \max _ { j \sim k , j \neq i } \left( \frac { w _ { i } + w _ { j } } { C _ { i j } } \right)
   $$
   其中，$k$为聚类数目，$W_i$表示$C_i$类中的所有样本到其聚类中心的平均距离，$W_i$表示$C_i$类中的所有样本到其聚类中心的平均距离，$C_{ij}$表示类$C_i$和$C_j$中心之间的距离。可以看出$DB$越小表示类与类之间的相似度越低，从而对应越佳的聚类结果。

3. Wint指标

   Wint指标是最大化类内相似度和最小化类间相似度，通常采用带罚项$\frac{1-2k}{n}$的Wint指标进行类数的估计，其最大值对应的类数为最佳聚类数。其中$intra(i)$表示类内相似度，$inter(i,j)$表示类间相似度。
   $$
   \operatorname { Wint } ( k ) = 1 - \frac { 1 } { \sum _ { i = 1 } ^ { k } n _ { i } \times \operatorname { intra } ( i ) } \sum _ { i = 1 } ^ { k } \frac { n _ { i } } { n - n _ { j } } \sum _ { j = 1,j \neq i } ^ { k } n _ { j } \times \text {inter} ( i , j )
   $$

### 基于泛化能力评价指标的最佳聚类数确定方法

基于泛化能力的最佳聚类数确定方法是通过分类的思想来解决聚类问题，将无指导的聚类与有指导的学习结合起来，通过对不同k值得到的聚类结果泛化能力的比较得出最优聚类数，将聚类泛化能力的评价指标定义为GA指标（generalization ability），其公式为:
$$
G A ( k ) = \frac { n } { N _ { t e } }
$$
$GA$指标的具体计算方法如下:
1）将给定的数据集进行随机拆分，分为训练集$tr$和测试集$te$两部分；
2）分别对训练集和测试集进行K-均值聚类，聚类数都为$k$，分别得到训练集的聚类结果$tr1$和测试集的聚类结果$te2$
3）应用分类方法，对步骤2中得到的训练集聚类结果$tr1$进行学习，并根据学习到的判别函数对测试集中的样本进行判别，判别结果为$te2$
4）比较$tel$和$te2$中对应样本的类别，计算$tel$和te2中类别相同的样本个数$n$占测试集样本总数$N$的比例，该比例称为$GA$指标，取值区间为$[0,1]$

不同于传统的聚类有效性指标，GA指标不是从聚类结果的结构来评价聚类效果，而是应用人工神经网络中的泛化能力来衡量聚类的有效性，GA指数越大，则说明该聚类泛化能力越高，聚类效果更佳。然而该指标不适用于聚类数为1的聚类，此时GA指数为1，虽然达到了最大，但是此时的聚类本身是没有意义的。

另外，在实际聚类中，聚类数不应过多，否则对于聚类结果将难以解释，因此对于有限的可选聚类数，可以采用穷举法得到。通过计算不同聚类数下的GA指数，选择最大的GA指数对应的聚类数作为整体数据集的最佳聚类数，并对原始数据整体进行聚类，得到最优的聚类结果。

### 自适应优化方法

基本思想如下：

1. 在一开始给定一个适合的数值给k，通过一次K-means算法得到一次聚类中心。

2. 对于得到的聚类中心，根据得到的k个聚类的距离情况，合并距离最近的类，因此聚类中心数减小，当将其用于下次聚类时，相应的聚类数目也减小了，最终得到合适数目的聚类数。
3. 可以通过一个评判值E来确定聚类数得到一个合适的位置停下来，而不继续合并聚类中心。重复上述循环，直至评判函数收敛为止，最终得到较优聚类数的聚类结果。

 ## 参考资料

[1] https://www.cnblogs.com/CBDoctor/archive/2011/10/24/2222358.html

[2] https://www.cnblogs.com/lyr2015/p/7436257.html

[3] 张雄，赵礼峰.基于泛化能力的K-均值最佳聚类数确定方法[D].南京邮电大学，2017.

[4] 李芳.K-Means算法的k值自适应优化方法研究[D].安徽大学，2015.

[5] 贾瑞玉，宋建林.基于聚类中心优化的k-means最佳聚类数确定方法[J].微电子学与计算机，2016，33(5)：62-66.