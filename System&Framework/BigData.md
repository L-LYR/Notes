# 大数据导论 ~学习笔记~

### 大数据

#### 数据

- 存储在某种介质上能够识别的物理符号。
  - 数据获取：物理信号至可存储数据
  - 数据存储：存储介质以及组织与管理
  - 数据使用：针对某个具体的应用目标
  - 任何一个数据被记录、存储一定有其最原始的价值期望，而一旦原始价值被实现，数据事实就是以一种成本存在。

#### 基本共识

- 大数据泛指对无法在可容忍的时间内用传统信息技术和软硬件工具对其进行获取、管理和处理的巨量数据集合，具有海量性、多样性、时效性及可变性等特征，需要可伸缩的计算体系结构以支持其存储、处理和分析。

#### 基本特征

- Volume Variety Velocity Value
  - 数据量级大，数据规模大
  - 数据本身所具备的多模式性、多模态性、异构性
  - 价值实现依赖一系列技术合集
  - 各边对大数据价值的共识和期望，针对不同利益的理解和关注点

#### 开源工具

- 数据存储
- 开发平台
- 开发工具和集成
- 分析和报告工具

#### 技术要点

- 数据的采集和汇集
  - 数据层的普适“多元、异构、跨时空”的典型特征
  - 需基于不同数据协议进行数据提取和交换
  - 原始开发团队缺位导致文档缺失、数据库封闭致使数据交换协议缺失
  - 不同利益间无商务合作
  - 任一数据源数据的存在具有其最原始的价值期望
  - 每一个数据源表示的物理对象不一致
  - 数据源的数据建设依托于不同IT的实施思路和建设水平
  - 对数据的有效特征提取、语义理解和融合
- 数据存储和管理
  - 持续增长的种类繁多的非结构化的海量数据
- 数据处理和分析
  - 在领域知识已经丰富完备的前提下，以逻辑为基础，利用领域知识对数据进行加工处理，然后直接为应用服务。
  - 以机器学习和数据挖掘为基础，通过对历史数据进行建模获得知识，然后利用此知识对数据进行加工处理，然后直接为应用服务。以下问题：
    - 数据及标签的动态变化性
    - 专家标注样本成本过高
  - 将上述的基于领域知识和数据驱动的知识发现有效地结合在一起，从而达到双边互补的同时，更好地应用服务
- 计算环境
  - 数据类型的复杂性给数据的理解、建模带来的挑战
  - 更快的计算效率对数据的海量、并行及快速更新的特性
  - 目标：新型的理论、算法、技术以及合适的高性能计算架构
  - 策略：
    - 充分提升和挖掘单个计算节点的计算性能（硬件方面）
    - 利用图形处理器GPU技术大幅提高单台计算设备的计算性能
    - 分治法，分布式计算架构

#### 研究范式

- 范式指的是从事某一科学的研究者群体对本体论、认识论和方法论的基本承诺，是科学家所共同接受的一组假说、理论、准则和方法的总和，这些东西形成科学家心理上的共同信念。
  - 实验归纳
    - 以记录和描述自然现象为特征
    - 以归纳为主，带有较多盲目性地观测和实验
  - 模型推演
    - 偏重理论总结和理性概括，强调普遍理论认识而非直接实用意义的科学
    - 以演绎法为主，不局限与描述经验事实
    - 强调数据模型的构建，超越实验设计
  - 仿真模拟
    -  对复杂现象通过模拟仿真，从而推演出越来越多复杂的现象
    - 涵盖数值模拟、模拟拟合与数据分析、计算优化
    - 本质上是假设推动的，即先提出可能的理论，再搜集数据，然后通过计算来验证
    - 计算机作为工具，其优势在于高速计算
  - 数据密集型科学发现
    - 对已有的并不断地有意识地富集的大量数据的有效分析和计算，得出未知结论
    - 让计算机自己从海量数据中发现模式，即共性客观
    - 不像实验归纳那么明确，不像理论和模拟那样有因果关系，主要是一种“大概”，即数据背后的客观事实

### 感知与获取

#### 数据源

- 归属区分：本单位自营与外单位他营
- 互联网数据
- 内部数据
  - 政府数据
  - 各利益主体自营数据
  - 物联网数据

#### 内部数据------获取与整合------ETL(Extract Transform and Load)

- 数据抽取

  - 全量抽取：对数据库的所有数据进行抽取，无复杂处理，较为直观、简单，但抽取效率低
  - 增量抽取：只抽取上次抽取以来数据库中新增或修改的数据
    - 日志比对：分析数据库自身的日志来判断变化的数据，如利用Oracle数据库的CDC(changed data capture)特性，在插入、更新或删除对数据进行提取
    - 时间戳：增加时间戳字段，更新修改表数据时，同时修改时间戳字段的值，该方法性能较好，但对系统有侵入性，而且无法捕获对时间戳以前数据的删除和更新操作
    - 触发器：三个触发器，插入、修改、删除，每当数据发生变化，就通过相应的触发器将变化的数据写入临时表，抽取线程从临时表中抽取数据，其中被抽去过的数据则会被标记或删除
    - 全表比对：使用MD5校验码，侵入性较小，但性能较差，而且对于重复记录以及无主键的表，准确性较差

- 数据清洗和转换

- 具体内容：

  数据清洗的任务是过滤那些不符合要求的数据，将过滤的结果交给业务主管部门，确认是否过滤掉还是由业务单位修正之后再进行抽取。

  1. 不完整的数据：这一类数据主要是一些应该有的信息缺失，如供应商的名称、分公司的名称、客户的区域信息缺失、业务系统中主表与明细表不能匹配等。补全后才写入数据仓库。

  2. 错误的数据：业务系统不够健全，在接收输入后没有进行判断直接写入后台数据库造成的，比如数值数据输成全角数字字符、字符串数据后面有一个回车操作、日期格式不正确、日期越界等。这一类数据也要分类，对于类似于全角字符、数据前后有不可见字符的问题，**只能通过写SQL语句的方式找出来，然后要求客户在业务系统修正之后抽取**。日期格式不正确的或者是日期越界的这一类**错误会导致ETL运行失败，**这一类错误需要去业务系统数据库用SQL的方式挑出来，修正之后再抽取。

  3. 重复的数据：对于这一类数据——特别是维度表中会出现这种情况——将重复数据记录的所有字段导出来，让客户确认并整理。

  反复的过程，对于是否过滤，是否修正一般要求客户确认，对于过滤掉的数据，写入Excel文件或者将过滤数据写入数据表，数据清洗需要注意的是不要将有用的数据过滤掉，对于每个过滤规则认真进行验证，并要用户确认。

  数据转换的任务主要进行不一致的数据转换、数据粒度的转换，以及一些商务规则的计算。

  1. 不一致数据转换：整合的过程，将不同业务系统的相同类型的数据统一，比如同一个供应商在结算系统的编码是XX0001，而在CRM中编码是YY0001，这样在抽取过来之后统一转换成一个编码。

  2. 数据粒度的转换：业务系统一般存储非常明细的数据，而数据仓库中数据是用来分析的，不需要非常明细的数据。一般情况下，**会将业务系统数据按照数据仓库粒度进行聚合。**

  3. 商务规则的计算：不同的企业有不同的业务规则、不同的数据指标，需要在ETL中将这些数据指标计算好了之后存储在数据仓库中，以供分析使用。

- - ETL引擎中
    - 字段映射
    - 数据过滤
    - 数据清洗
    - 数据替换
    - 数据计算
    - 数据验证
    - 数据加解密
    - 数据合并
    - 数据拆分
  - 数据库中
    - SQL指令和函数

- 数据加载

  - 直接用SQL语句进行插入、更新、删除操作（有日志记录，可恢复）
  - 采用批量装载方法，用一些批量装载工具或API（效率高）

#### 外部数据------获取------爬虫

- 分类
  - 批量型：评估算法可行性以及目标URL数据可用性，以下两种的基础
  - 增量型：准时或实时获取互联网数据的任何应用场景
  - 垂直型：准时或实时获取互联网中与指定内容相关的数据
- 数据基础：URL池
- 抓取策略：
  - 深度优先策略
  - 广度优先策略
  - 局部PageRank策略
  - OPIC策略(Online Page Importance Computation)
- 基本步骤：
  - 从URL池中选择某个URL
  - 读取该URL对应的专有抓取策略，按策略抓取URL内容
  - 按约定的爬取策略选择下一个URL
- 评价标准：
  - 高效性、可扩展性、健壮性、友好性、抓取网页的覆盖率、及时性、重要性
- 开源工具

